{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65bedc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cddc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7096109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Temperature array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start temp\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # Set shower length\n",
    "        self.shower_length = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 temperature\n",
    "        # 1 -1 = 0 \n",
    "        # 2 -1 = 1 temperature \n",
    "        self.state += action -1 \n",
    "        # Reduce shower length by 1 second\n",
    "        self.shower_length -= 1 \n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.state >=37 and self.state <=39: \n",
    "            reward =1 \n",
    "        else: \n",
    "            reward = -1 \n",
    "        \n",
    "        # Check if shower is done\n",
    "        if self.shower_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Apply temperature noise\n",
    "        #self.state += random.randint(-1,1)\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # Reset shower time\n",
    "        self.shower_length = 60 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33db8f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env = ShowerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b1a6e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79.04312], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ecddbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-56\n",
      "Episode:2 Score:-28\n",
      "Episode:3 Score:-28\n",
      "Episode:4 Score:-44\n",
      "Episode:5 Score:-50\n",
      "Episode:6 Score:-44\n",
      "Episode:7 Score:16\n",
      "Episode:8 Score:-30\n",
      "Episode:9 Score:2\n",
      "Episode:10 Score:-56\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbf543ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms import ppo\n",
    "import ray\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ed9df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:12:51,621\tINFO worker.py:1622 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.7.15</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.4.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.7.15', ray_version='2.4.0', ray_commit='4479f66d4db967d3c9dd0af2572061276ba926ba', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': 'tcp://127.0.0.1:56912', 'raylet_socket_name': 'tcp://127.0.0.1:57801', 'webui_url': '127.0.0.1:8265', 'session_dir': 'D:\\\\TempFiles\\\\ray\\\\session_2023-04-30_22-12-49_783293_24416', 'metrics_export_port': 61447, 'gcs_address': '127.0.0.1:60970', 'address': '127.0.0.1:60970', 'dashboard_agent_listen_port': 52365, 'node_id': '3c59b31eae6eb4b5ab7463e59b6743392f4c5a5f152bd16874680758'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7451a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--local-mode'], dest='local_mode', nargs=0, const=True, default=False, type=None, choices=None, help='Init Ray in local mode for easier debugging.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\", type=str, default=\"PPO\", help=\"The RLlib-registered algorithm to use.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "    default=\"tf2\",\n",
    "    help=\"The DL framework specifier.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\", type=int, default=50, help=\"Number of iterations to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\", type=int, default=100000, help=\"Number of timesteps to train.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\", type=float, default=0.1, help=\"Reward at which we stop training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-tune\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Run without Tune using a manual train loop instead. In this case,\"\n",
    "    \"use PPO without grid search and no TensorBoard.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f260fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:13:01,794\tWARNING deprecation.py:51 -- DeprecationWarning: `algo = Algorithm(env='<class '__main__.ShowerEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class '__main__.ShowerEnv'>').build()` instead. This will raise an error in the future!\n",
      "2023-04-30 22:13:01,846\tINFO algorithm.py:528 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\TempFiles\\ipykernel_24416\\2072079824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mShowerEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"env_config\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[0mlogger_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m         )\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout, sync_config)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_ip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_node_ip_address\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rollout_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m                 \u001b[0mlogdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m             )\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[0;32m    174\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                     \u001b[0mlocal_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m                 )\n\u001b[0;32m    178\u001b[0m             \u001b[1;31m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m_setup\u001b[1;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mspaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspaces\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m             )\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m_make_worker\u001b[1;34m(self, cls, env_creator, validate_env, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[0;32m    975\u001b[0m             \u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[0mspaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspaces\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m             \u001b[0mdataset_shards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ds_shards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m         )\n\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env_creator, validate_env, config, worker_index, num_workers, recreated_worker, log_dir, spaces, default_policy_class, dataset_shards, policy_config, input_creator, output_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, clip_rewards, normalize_actions, clip_actions, env_config, model_config, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, fake_sampler, seed, log_level, callbacks, disable_env_checking, policy_spec, policy_mapping_fn, policies_to_train, extra_python_environs, policy, tf_session_creator)\u001b[0m\n\u001b[0;32m    607\u001b[0m         ):\n\u001b[0;32m    608\u001b[0m             \u001b[1;31m# Run the `env_creator` function passing the EnvContext.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[0mclip_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m   2339\u001b[0m             \u001b[1;31m# All other env classes: Call c'tor directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0menv_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0menv_specifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         \u001b[1;31m# No env -> Env creator always returns None.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "algo = dqn.DQN(env=ShowerEnv, config ={\"env_config\": {},})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8292d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp37-cp37m-win_amd64.whl (162.6 MB)\n",
      "     ------------------------------------- 162.6/162.6 MB 13.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\donvi\\.conda\\envs\\droo\\lib\\site-packages (from torch) (4.5.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff16e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Discrete(3)\n",
    "    def reset(self):\n",
    "        return self.observation_space\n",
    "    def step(self, action):\n",
    "        reward = 5\n",
    "        done = 1\n",
    "        \n",
    "        return self.observation_space, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02033fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:17:07,730\tWARNING deprecation.py:51 -- DeprecationWarning: `algo = Algorithm(env='<class '__main__.MyEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class '__main__.MyEnv'>').build()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=13716)\u001b[0m WARNING:tensorflow:From C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=13716)\u001b[0m Instructions for updating:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=13716)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m   File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m     raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m ValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m \u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m Learn more about the most important changes here:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m In order to fix this problem, do the following:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m 1) Run `pip install gymnasium` on your command line.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m 2) Change all your import statements in your code from\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    `import gym` -> `import gymnasium as gym` OR\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m For your custom (single agent) gym.Env classes:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m 3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m      EnvCompatibility` wrapper class.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m 4.2) Alternatively to 4.1:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m  - Change your `reset()` method to have the call signature 'def reset(self, *,\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    seed=None, options=None)'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m  - Return an additional info dict (empty dict should be fine) from your `reset()`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    method.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m  - Return an additional `truncated` flag from your `step()` method (between `done` and\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    `info`). This flag should indicate, whether the episode was terminated prematurely\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    due to some time constraint or other kind of horizon setting.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m For your custom RLlib `MultiAgentEnv` classes:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m 4.1) Either wrap your old MultiAgentEnv via the provided\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m      `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m      MultiAgentEnvCompatibility` wrapper class.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m  - Change your `reset()` method to have the call signature\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    'def reset(self, *, seed=None, options=None)'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m  - Return an additional per-agent info dict (empty dict should be fine) from your\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    `reset()` method.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m  - Rename `dones` into `terminateds` and only set this to True, if the episode is really\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    done (as opposed to has been terminated prematurely due to some horizon/time-limit\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    setting).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m  - Return an additional `truncateds` per-agent dictionary flag from your `step()`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    method, including the `__all__` key (100% analogous to your `dones/terminateds`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    per-agent dict).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    flag should indicate, whether the episode (for some agent or all agents) was\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m    terminated prematurely due to some time constraint or other kind of horizon setting.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m   File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24420)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:17:13,362\tERROR actor_manager.py:508 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=6440, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000018A22D1F3C8>)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\n",
      "    raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\n",
      "ValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=6440, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000018A22D1F3C8>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 921, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 616, in __init__\n",
      "    check_env(self.env)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 93, in check_env\n",
      "    f\"{actual_error}\\n\"\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "    check_gym_environments(env)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\n",
      "    raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\n",
      "ValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n",
      "2023-04-30 22:17:13,363\tERROR actor_manager.py:508 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=13716, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002698784B7C8>)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\n",
      "    raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\n",
      "ValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=13716, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002698784B7C8>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 870, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 921, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 616, in __init__\n",
      "    check_env(self.env)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 93, in check_env\n",
      "    f\"{actual_error}\\n\"\n",
      "ValueError: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n",
      "    check_gym_environments(env)\n",
      "  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\n",
      "    raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\n",
      "ValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "\n",
      "The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n    check_gym_environments(env)\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\n    raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\nValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m                     \u001b[0mlocal_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m                 )\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m_setup\u001b[1;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_workers_after_construction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         )\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36madd_workers\u001b[1;34m(self, num_workers, validate)\u001b[0m\n\u001b[0;32m    634\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py\u001b[0m in \u001b[0;36m__fetch_result\u001b[1;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m                 \u001b[0mremote_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mResultOrError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\_private\\worker.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2522\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=6440, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000018A22D1F3C8>)\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\n    raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\nValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=6440, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000018A22D1F3C8>)\n  File \"python\\ray\\_raylet.pyx\", line 870, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 921, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 616, in __init__\n    check_env(self.env)\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 93, in check_env\n    f\"{actual_error}\\n\"\nValueError: Traceback (most recent call last):\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n    check_gym_environments(env)\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\n    raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\nValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\TempFiles\\ipykernel_24416\\3198692568.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m algo = ppo.PPO(env=MyEnv, config={\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;34m\"env_config\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# config to pass to env class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m })\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[0mlogger_creator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogger_creator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m         )\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout, sync_config)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_ip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_node_ip_address\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rollout_workers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m                 \u001b[0mlogdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m             )\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[0;32m    192\u001b[0m                     \u001b[1;31m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                     \u001b[1;31m# to a config mismatch) thrown inside the actor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# In any other case, raise the RayActorError as-is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 81, in check_env\n    check_gym_environments(env)\n  File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\utils\\pre_checks\\env.py\", line 138, in check_gym_environments\n    raise ValueError(ERR_MSG_OLD_GYM_API.format(env, \"\"))\nValueError: Your environment (<MyEnv instance>) does not abide to the new gymnasium-style API!\nFrom Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n\nLearn more about the most important changes here:\nhttps://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n\nIn order to fix this problem, do the following:\n\n1) Run `pip install gymnasium` on your command line.\n2) Change all your import statements in your code from\n   `import gym` -> `import gymnasium as gym` OR\n   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n\nFor your custom (single agent) gym.Env classes:\n3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n     EnvCompatibility` wrapper class.\n3.2) Alternatively to 3.1:\n - Change your `reset()` method to have the call signature 'def reset(self, *,\n   seed=None, options=None)'\n - Return an additional info dict (empty dict should be fine) from your `reset()`\n   method.\n - Return an additional `truncated` flag from your `step()` method (between `done` and\n   `info`). This flag should indicate, whether the episode was terminated prematurely\n   due to some time constraint or other kind of horizon setting.\n\nFor your custom RLlib `MultiAgentEnv` classes:\n4.1) Either wrap your old MultiAgentEnv via the provided\n     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n     MultiAgentEnvCompatibility` wrapper class.\n4.2) Alternatively to 4.1:\n - Change your `reset()` method to have the call signature\n   'def reset(self, *, seed=None, options=None)'\n - Return an additional per-agent info dict (empty dict should be fine) from your\n   `reset()` method.\n - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n   setting).\n - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n   per-agent dict).\n   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n   flag should indicate, whether the episode (for some agent or all agents) was\n   terminated prematurely due to some time constraint or other kind of horizon setting.\n\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env])."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m 2023-04-30 22:17:13,355\tERROR worker.py:844 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=13716, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002698784B7C8>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=13716, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002698784B7C8>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m   File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m   File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m   File \"C:\\Users\\donvi\\.conda\\envs\\DROO\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 616, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m     check_env(self.env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m     f\"{actual_error}\\n\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m     check_gym_environments(env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13716)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).\n"
     ]
    }
   ],
   "source": [
    "algo = ppo.PPO(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7eba32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DROO]",
   "language": "python",
   "name": "conda-env-.conda-DROO-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
